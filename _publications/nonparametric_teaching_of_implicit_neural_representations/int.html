<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Implicit Neural Teaching</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
<!--     <meta property="og:image" content="">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="512"> -->
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://chen2hang.github.io/_publications/nonparametric_teaching_of_implicit_neural_representations/"/>
    <meta property="og:title" content="Implicit Neural Teaching" />
    <meta property="og:description" content="Project page for Nonparametric Teaching of Implicit Neural Representations." />

        <!--TWITTER-->
<!--     <meta name="twitter:card" content="summary_large_image" /> -->
    <meta name="twitter:title" content="Implicit Neural Teaching" />
    <meta name="twitter:description" content="Project page for Nonparametric Teaching of Implicit Neural Representations." />
<!--     <meta name="twitter:image" content="https://people.eecs.berkeley.edu/~bmild/fourfeat/img/foxface.jpg" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <link rel="icon" type="image/png" href="/images/site_icon_hku.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
<!--     <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css"> -->
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
<!--     <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110862391-1');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script> -->
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Nonparametric Teaching of Implicit Neural Representations</br> 
                <small>
                    ICML 2024
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://chen2hang.github.io">
                          Chen Zhang*
                        </a>
                        </br>HKU
                    </li>
                    <li>
                        <a href="https://www.cs.toronto.edu/~stevenlts/">
                          Steven Tin Sui Luo*
                        </a>
                        </br>UoT
                    </li>
                    <li>
                        <a href="https://www.researchgate.net/profile/Jason_Chun_Lok_Li">
                            Jason Chun Lok Li
                        </a>
                        </br>HKU
                    </li>
                    <li>
                        <a href="https://www.eee.hku.hk/~ycwu/">
                          Yik-Chung Wu
                        </a>
                        </br>HKU
                    </li>
                    <li>
                        <a href="https://www.eee.hku.hk/~nwong/">
                          Ngai Wong
                        </a>
                        </br>HKU
                    </li>
                </ul>
                * denotes equal contribution
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="/_publications/nonparametric_teaching_of_implicit_neural_representations/ICML_2024_Paper.pdf">
                            <image src="paper_thumbnail.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="/_publications/nonparametric_teaching_of_implicit_neural_representations/ICML_2024_Poster.pdf">
                            <image src="poster_thumbnail.png" height="60px">
                                <h4><strong>Poster</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="/_publications/nonparametric_teaching_of_implicit_neural_representations/ICML_2024_Slides.pdf">
                            <image src="slides_thumbnail.png" height="60px">
                                <h4><strong>Slides</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/stevolopolis/nmt_inr">
                            <image src="github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="figure1.png" style="display: block; margin: 0 auto;" alt="workflow" height="400px"><br>
                <p class="text-justify">
We investigate the learning of implicit neural representation (INR) using an overparameterized multilayer perceptron (MLP) via a novel nonparametric teaching perspective. The latter offers an efficient example selection framework for teaching nonparametrically defined (viz. non-closed-form) target functions, such as image functions defined by 2D grids of pixels. To address the costly training of INRs, we propose a paradigm called <b>Implicit Neural Teaching</b> (INT) that treats INR learning as a nonparametric teaching problem, where the given signal being fitted serves as the target function. The teacher then selects signal fragments for iterative training of the MLP to achieve fast convergence. By establishing a connection between MLP evolution through parameter-based gradient descent and that of function evolution through functional gradient descent in nonparametric teaching, we show <i>for the first time</i> that teaching an overparameterized MLP is consistent with teaching a nonparametric learner. This new discovery readily permits a convenient drop-in of nonparametric teaching algorithms to broadly enhance INR training efficiency, demonstrating 30%+ training time savings across various input modalities.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <h4>A video showing results</h2>
                        <table border="1">
                            <tr>
                                <td>Target Image</td>
                                <td>Arbitrary 1 shown in Figure 10</td>
                                <td>Arbitrary 1 shown in Figure 10</td>
                                <td>Arbitrary 1 shown in Figure 10</td>
                                <td></td>
                            </tr>
                            <tr>
                                <td>Learnt Image</td>
                                <td>..</td>
                                <td>..</td>
                                <td>..</td>
                                <td>..</td>
                            </tr>
                            <tr>
                                <td>Selected Fragments</td>
                                <td>..</td>
                                <td>..</td>
                                <td>..</td>
                                <td>..</td>
                            </tr>
                        </table>
<!--                         <iframe src="https://www.youtube.com/embed/nVA6K6Sn2S4" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe> -->
                    </div>
                </div>
            </div>
        </div>


<!--        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Training a network without and with Fourier features
                </h3>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/lion_none_gauss_v1.mp4" type="video/mp4" />
                </video>
                <p class="text-justify">
                    In this paper, we train MLP networks to learn <em>low dimensional</em> functions, such as the function defined by an image that maps each (x, y) pixel coordinate to an output (r, g, b) color. A standard MLP is not able to learn such functions (blue border image). Simply applying a Fourier feature mapping to the input (x, y) points before passing them to the network allows for rapid convergence (orange border image). 
                </p>
                <p class="text-justify">
                    This Fourier feature mapping is very simple. For an input point <b>v</b> (for the example above, (x, y) pixel coordinates) and a random Gaussian matrix <b>B</b>, where each entry is drawn independently from a normal distribution N(0, σ<sup>2</sup>), we use
                </p>
                <p style="text-align:center;">
                    <image src="img/gamma.png" height="30px" class="center">
                </p>
                <p class="text-justify">
                    to map input coordinates into a higher dimensional feature space before passing them through the network. 
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Fourier features and the Neural Tangent Kernel
                </h3>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="img/test_sweep_1e-4_5000_more_low.mp4" type="video/mp4" />
                </video>
                <p class="text-justify">
                    Recent theoretical work describes the behavior of deep networks in terms of the <em>neural tangent kernel</em> (NTK), showing that the network's predictions over the course of training closely track the outputs of kernel regression problem being optimized by gradient descent. In our paper, we show that using a Fourier feature mapping transforms the NTK into a stationary kernel in our low-dimensional problem domains. In this context, the bandwidth of the NTK limits the spectrum of the recovered function. 
                </p>
                <p class="text-justify">
                    In the video above, we show how scaling the Fourier feature frequencies provides direct control over the width of the NTK. This allows us to traverse a regime from underfitting (low scale, recovered function too low frequency) to overfitting (high scale, recovered function too high frequency), with the best generalization performance in the middle. Note that each image shown is the output of a different trained MLP network. The networks are supervised on a subsampled 256 x 256 image and tested at the full 512 x 512 resolution.
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    Random Fourier features were first proposed in the seminal work of <a href="https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf">Rahimi & Recht (2007)</a>.
                </p>
                <p class="text-justify">
                    The neural tangent kernel was introduced in <a href="https://arxiv.org/abs/1806.07572">Jacot et al. (2018)</a>. 
                </p>
                <p class="text-justify">
                    We relied on the excellent open source projects <a href="https://github.com/google/jax">JAX</a> and <a href="https://github.com/google/neural-tangents">Neural Tangents</a> for training networks and calculating neural tangent kernels.
                </p>
                <p class="text-justify">
                    In own previous work on <em>neural radiance fields</em> (<a href="https://www.matthewtancik.com/nerf">NeRF</a>), we were surprised to find that a "positional encoding" of input coordinates helped networks learn significantly higher frequency details, inspiring our exploration in this project.
                </p>
                <p class="text-justify">
                    <a href="https://vsitzmann.github.io/siren/">Sitzmann et al. (2020)</a> concurrently introduced <em>sinusoidal representation networks</em> (SIREN), demonstrating exciting progress in coordinate based MLP representations by using a sine function as the nonlinearity between <em>all</em> layers in the network. This allows the MLPs to accurately represent first and second order derivatives of low dimensional signals. 
                </p>
                <p class="text-justify">
                    You can find code to replicate all our experiments on <a href="https://github.com/tancik/fourier-feature-networks">GitHub</a>, but if you just want to try experimenting with the images used on this webpage you can find the uncompressed originals here: 
                    <a href="img/lion_orig.png">Lion</a>,
                    <a href="img/greece_orig.png">Greece</a>,
                    <a href="img/fox_orig.png">Fox</a>.
                </p>
            </div>
        </div> -->
        

<!--         <div class="row" id="header_img">
            <figure class="col-md-8 col-md-offset-2">
                <image src="img/llff_teaser.png" class="img-responsive" alt="overview">
                <figcaption>
                </figcaption>
            </figure>
                
        </div> -->
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly style="width: 550px; height: 150px;">
@InProceedings{zhang2024ntinr,
    title={Nonparametric Teaching of Implicit Neural Representations},
    author={Zhang, Chen and Luo, Steven and Li, Jason and Wu, Yik-Chung and Wong, Ngai},
    booktitle = {ICML},
    year={2024}
}</textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                We thank all anonymous reviewers for their constructive feedback to improve this project.
                    <br>
                This work was supported by the Theme-based Research Scheme (TRS) project T45-701/22-R, and in part by ACCESS – AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong SAR.
                    <br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
